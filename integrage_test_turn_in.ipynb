{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import  OneHotEncoder\n",
    "from category_encoders import BinaryEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('orange_small_test.data', sep = '\\t',header=None)\n",
    "test_appetency = pd.read_csv('orange_small_train_appetency_test.labels',header=None).astype('float')\n",
    "test_churn = pd.read_csv('orange_small_train_churn_test.labels',header=None).astype('float')\n",
    "test_upselling = pd.read_csv('orange_small_train_upselling_test.labels',header=None).astype('float')\n",
    "\n",
    "test_appetency.columns = ['appetency']\n",
    "test_churn.columns = ['churn']\n",
    "test_upselling.columns = ['upselling']\n",
    "test_churn = (test_churn + 1)/2\n",
    "test_upselling = (test_upselling + 1)/2\n",
    "test_appetency = (test_appetency + 1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_info_dic = {}\n",
    "with open('train_info_dic.pkl','rb') as f:\n",
    "    train_info_dic=pickle.load(f,encoding='bytes')\n",
    "\n",
    "test_data.columns = train_info_dic['orig_columns']\n",
    "test_data_drop_na = test_data.drop(train_info_dic['drop_list'],axis=1)\n",
    "test_data_light_gbm = test_data_drop_na.copy()\n",
    "category_columns = test_data_drop_na.select_dtypes(['object']).columns\n",
    "test_data_light_gbm[category_columns] = test_data_drop_na[category_columns].apply(lambda col: col.astype('category'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_columns = train_info_dic['category_columns']\n",
    "test_data_drop_na[category_columns] = test_data_drop_na[category_columns].apply(lambda col: col.astype('category'))\n",
    "\n",
    "test_data_drna_mean = test_data_drop_na.fillna(train_info_dic['train_data_mean'])\n",
    "\n",
    "test_data_drna_mean_scale = test_data_drna_mean\n",
    "numberic_columns_list = train_info_dic['numberic_columns']  \n",
    "std_scaler = train_info_dic['std_scaler'] \n",
    "test_data_drna_mean_scale[numberic_columns_list] = std_scaler.transform(test_data_drna_mean[numberic_columns_list])\n",
    "\n",
    "binary_encoder = train_info_dic['binary_encoder']\n",
    "test_data_drna_mean_scale_bin = binary_encoder.transform(test_data_drna_mean_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimp = train_info_dic['imp']\\nnumberic_columns = test_data_drop_na.select_dtypes(exclude=['category']).columns\\ntest_data_drop_na_imputed = imp.transform(test_data_drop_na[numberic_columns])\\ntest_data_drop_na[numberic_columns] = pd.DataFrame(test_data_drop_na_imputed)\\ntest_data_drna_mean = test_data_drop_na\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "imp = train_info_dic['imp']\n",
    "numberic_columns = test_data_drop_na.select_dtypes(exclude=['category']).columns\n",
    "test_data_drop_na_imputed = imp.transform(test_data_drop_na[numberic_columns])\n",
    "test_data_drop_na[numberic_columns] = pd.DataFrame(test_data_drop_na_imputed)\n",
    "test_data_drna_mean = test_data_drop_na\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.96      0.97     24548\n",
      "         1.0       0.11      0.25      0.15       452\n",
      "\n",
      "    accuracy                           0.95     25000\n",
      "   macro avg       0.55      0.61      0.56     25000\n",
      "weighted avg       0.97      0.95      0.96     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_rf_appetency = joblib.load(\"random_forest_appetency.model\")\n",
    "result_rf_appetency = model_rf_appetency.predict(test_data_drna_mean_scale_bin)\n",
    "print(classification_report(test_appetency,result_rf_appetency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.80      0.87     23165\n",
      "         1.0       0.16      0.46      0.24      1835\n",
      "\n",
      "    accuracy                           0.78     25000\n",
      "   macro avg       0.55      0.63      0.55     25000\n",
      "weighted avg       0.89      0.78      0.82     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_rf_churn = joblib.load(\"random_forest_churn.model\")\n",
    "result_rf_churn = model_rf_churn.predict(test_data_drna_mean_scale_bin)\n",
    "print(classification_report(test_churn,result_rf_churn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.99      0.97     23181\n",
      "         1.0       0.77      0.47      0.58      1819\n",
      "\n",
      "    accuracy                           0.95     25000\n",
      "   macro avg       0.86      0.73      0.78     25000\n",
      "weighted avg       0.95      0.95      0.95     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_rf_upselling = joblib.load(\"random_forest_upselling.model\")\n",
    "result_rf_upselling = model_rf_upselling.predict(test_data_drna_mean_scale_bin)\n",
    "print(classification_report(test_upselling,result_rf_upselling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.97      0.98     24548\n",
      "         1.0       0.12      0.23      0.16       452\n",
      "\n",
      "    accuracy                           0.95     25000\n",
      "   macro avg       0.55      0.60      0.57     25000\n",
      "weighted avg       0.97      0.95      0.96     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_lightgbm_appetency = joblib.load(\"light_gbm_appetency.model\")\n",
    "result_lgb_appetency = model_lightgbm_appetency.predict(test_data_light_gbm)\n",
    "print(classification_report(test_appetency,result_lgb_appetency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.94      0.94     23165\n",
      "         1.0       0.21      0.20      0.20      1835\n",
      "\n",
      "    accuracy                           0.88     25000\n",
      "   macro avg       0.57      0.57      0.57     25000\n",
      "weighted avg       0.88      0.88      0.88     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_lightgbm_churn = joblib.load(\"light_gbm_churn.model\")\n",
    "result_lgb_churn = model_lightgbm_churn.predict(test_data_light_gbm)\n",
    "print(classification_report(test_churn,result_lgb_churn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.97      0.95     23165\n",
      "         1.0       0.24      0.12      0.16      1835\n",
      "\n",
      "    accuracy                           0.91     25000\n",
      "   macro avg       0.59      0.54      0.55     25000\n",
      "weighted avg       0.88      0.91      0.89     25000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.98      0.97     23181\n",
      "         1.0       0.62      0.48      0.54      1819\n",
      "\n",
      "    accuracy                           0.94     25000\n",
      "   macro avg       0.79      0.73      0.75     25000\n",
      "weighted avg       0.93      0.94      0.94     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_lightgbm_upselling = joblib.load(\"light_gbm_upselling.model\")\n",
    "result_lgb_upselling = model_lightgbm_upselling.predict(test_data_light_gbm)\n",
    "print(classification_report(test_upselling,result_lgb_upselling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_new = result_rf + result_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_new[result_new==2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_new[result_new==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covert_result(x):\n",
    "    if x>=2:\n",
    "        x=1\n",
    "    else:\n",
    "        x=0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_covert_result = np.vectorize(covert_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_result = vec_covert_result(result_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_appetency,vec_result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
